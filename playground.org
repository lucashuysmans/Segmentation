# --------------------------
#+TITLE: Notebook Playground
# --------------------------


* PyTorch

** Introduction to Tensors

Tensors in torch are handled like ndarray in numpy.

#+begin_src jupyter-python
import torch
import numpy as np

# initialisation from data
data = [[1, 2],[3, 4]]
x_data = torch.tensor(data)
x_ones = torch.ones_like(x_data)

tensor = torch.rand(3,4)

print(f"Shape of tensor: {tensor.shape}")
print(f"Datatype of tensor: {tensor.dtype}")
print(f"Device tensor is stored on: {tensor.device}")
#+end_src

#+RESULTS:
: Shape of tensor: torch.Size([3, 4])
: Datatype of tensor: torch.float32
: Device tensor is stored on: cpu


Optionally, one can move the tensor to the GPU resources using cuda.

#+begin_src jupyter-python
if torch.cuda.is_available():
  tensor = tensor.to('cuda')

print(f"Device tensor is stored on: {tensor.device}")
#+end_src

#+RESULTS:
: Device tensor is stored on: cpu


** Initialising a Model and Backward Propagation

In the following we take a pretrained ~resnet~ and show how to perform a single training step.

#+begin_src jupyter-python
import torch, torchvision
model = torchvision.models.resnet18(pretrained = True)
data = torch.rand(1,3,64,64)
labels = torch.rand(1,1000)
#+end_src

#+RESULTS:


In order to pass the data into the model (/forward pass/) we can call the object like a function. The error can be simply computer by comparing with a given tensor with the correct output format.

#+begin_src jupyter-python
prediction = model(data)
loss = (prediction - labels).sum()
loss.backward()                 # calculates gradient w.r.t. parameters and
                                # stores it in the loss.grad attribute
#+end_src

#+RESULTS:


This procedure can be automatised using "optimisers". One example is stochastic gradient descent (~torch.optim.SGD~).

#+begin_src jupyter-python
optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)
optim.step()
#+end_src

#+RESULTS:


*** Autograd

The following just gives a few examples of how pytorch manages gradients, backward propagation, etc.

#+begin_src jupyter-python
import torch

a = torch.tensor([2., 3.], requires_grad=True)
b = torch.tensor([6., 4.], requires_grad=True)

Q = 3*a**2 - b**2

test = torch.tensor([1., 1.])
Q.backward(gradient=test)

print(9*a**2 == a.grad)
print(-2*b == b.grad)
#+end_src

#+RESULTS:
: tensor([False, False])
: tensor([True, True])



** Neural Networks
#+begin_src jupyter-python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        # 1 input image channel, 6 output channels, 3x3 square convolution
        # kernel
        self.conv1 = nn.Conv2d(1, 6, 3)
        self.conv2 = nn.Conv2d(6, 16, 3)
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # If the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


net = Net()
print(net)
#+end_src

#+RESULTS:
: Net(
:   (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))
:   (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))
:   (fc1): Linear(in_features=576, out_features=120, bias=True)
:   (fc2): Linear(in_features=120, out_features=84, bias=True)
:   (fc3): Linear(in_features=84, out_features=10, bias=True)
: )
