# --------------------------
#+TITLE: Notebook Playground
# --------------------------

#+LATEX_CLASS: scrartcl

* PyTorch

** Introduction to Tensors

Tensors in torch are handled like ndarray in numpy.

#+begin_src jupyter-python
import torch
import numpy as np

# initialisation from data
data = [[1, 2],[3, 4]]
x_data = torch.tensor(data)
x_ones = torch.ones_like(x_data)

tensor = torch.rand(3,4)

print(f"Shape of tensor: {tensor.shape}")
print(f"Datatype of tensor: {tensor.dtype}")
print(f"Device tensor is stored on: {tensor.device}")
#+end_src

#+RESULTS:
: Shape of tensor: torch.Size([3, 4])
: Datatype of tensor: torch.float32
: Device tensor is stored on: cpu


Optionally, one can move the tensor to the GPU resources using cuda.

#+begin_src jupyter-python
if torch.cuda.is_available():
  tensor = tensor.to('cuda')

print(f"Device tensor is stored on: {tensor.device}")
#+end_src

#+RESULTS:
: Device tensor is stored on: cpu


** Initialising a Model and Backward Propagation

In the following we take a pretrained ~resnet~ and show how to perform a single training step.

#+begin_src jupyter-python
import torch, torchvision
model = torchvision.models.resnet18(pretrained = True)
data = torch.rand(1,3,64,64)
labels = torch.rand(1,1000)
#+end_src

#+RESULTS:


In order to pass the data into the model (/forward pass/) we can call the object like a function. The error can be simply computer by comparing with a given tensor with the correct output format.

#+begin_src jupyter-python
prediction = model(data)
loss = (prediction - labels).sum()
loss.backward()                 # calculates gradient w.r.t. parameters and
                                # stores it in the loss.grad attribute
#+end_src

#+RESULTS:


This procedure can be automatised using "optimisers". One example is stochastic gradient descent (~torch.optim.SGD~).

#+begin_src jupyter-python
optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)
optim.step()
#+end_src

#+RESULTS:


*** Autograd

The following just gives a few examples of how pytorch manages gradients, backward propagation, etc.

#+begin_src jupyter-python
import torch

a = torch.tensor([2., 3.], requires_grad=True)
b = torch.tensor([6., 4.], requires_grad=True)

Q = 3*a**2 - b**2

test = torch.tensor([1., 1.])
Q.backward(gradient=test)

print(9*a**2 == a.grad)
print(-2*b == b.grad)
#+end_src

#+RESULTS:
: tensor([False, False])
: tensor([True, True])



** Neural Networks

The networks can be built "by hand", on top of the nn.Module class. In the following example, the different layers are initialised as attributes of the network's class. Then, a ~forward~ method is implemented, which automatically implies a ~backward~ method. The ~forward~ method describes how the different layers interact with each other.

#+begin_src jupyter-python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        # 1 input image channel, 6 output channels, 3x3 square convolution
        # kernel
        self.conv1 = nn.Conv2d(1, 6, 3)
        self.conv2 = nn.Conv2d(6, 16, 3)
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # If the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


net = Net()
print("The net's different layers:")
print(net)
#+end_src

#+RESULTS:
: The net's different layers:
: Net(
:   (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))
:   (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))
:   (fc1): Linear(in_features=576, out_features=120, bias=True)
:   (fc2): Linear(in_features=120, out_features=84, bias=True)
:   (fc3): Linear(in_features=84, out_features=10, bias=True)
: )


This sends a random input (32x32 bitmap) through out network and propagates the gradient of the output (tested against some random tensor) back through the network. If one wants to compute the backpropagated parameters this way, one has to be a bit careful and initialise the gradient to zero beforehand (~zero_grad~). Otherwise, the gradient for successive calls of ~backward~ are accumulated (is this correct?).

#+begin_src jupyter-python
input = torch.randn(1,1, 32, 32)
out = net(input)

print(out)
print(out.size())
net.zero_grad()
out.backward(torch.randn(1,10))
#+end_src

#+RESULTS:
: tensor([[ 0.0136, -0.0787, -0.1801, -0.0942,  0.0553,  0.0932,  0.0784, -0.0177,
:          -0.0566, -0.0199]], grad_fn=<AddmmBackward>)
: torch.Size([1, 10])


More practically, one would like to calculate the gradient of a loss function with respect to some target. The following implements this using a random target and ~MSELoss~. It also inspects the computational graph that is implicitly stored with ~loss~. This is used for backpropagation.

#+begin_src jupyter-python
output = net(input)
target = torch.randn(10)  # a dummy target, for example
target = target.view(1, -1)  # make it the same shape as output
criterion = nn.MSELoss()

loss = criterion(output, target)
print(loss)
print(loss.grad_fn)
print(loss.grad_fn.next_functions)
print(loss.grad_fn.next_functions[0][0].next_functions)
#+end_src

#+RESULTS:
: tensor(0.8583, grad_fn=<MseLossBackward>)
: <MseLossBackward object at 0x7ff81262f280>
: ((<AddmmBackward object at 0x7ff81261d310>, 0), (None, 0))
: ((<AccumulateGrad object at 0x7ff81262f280>, 0), (<ReluBackward0 object at 0x7ff81261dd00>, 0), (<TBackward object at 0x7ff81261dac0>, 0))


The above code demonstrated how we can obtain the gradient of a loss function with respect to the networks parameters. In order to update the parameters, one may use a naive gradient descent:

#+begin_src jupyter-python
learning_rate = 0.01
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)
#+end_src


Alternatively, more elaborate optimisation schemes are available under the ~torch.optim~ package. Here, this is illustrated for stochastic gradient descent:

#+begin_src jupyter-python
import torch.optim as optim

# create your optimizer
optimizer = optim.SGD(net.parameters(), lr=0.01)

# in your training loop:
optimizer.zero_grad()   # zero the gradient buffers
output = net(input)
loss = criterion(output, target)
loss.backward()
optimizer.step()    # Does the update
#+end_src
