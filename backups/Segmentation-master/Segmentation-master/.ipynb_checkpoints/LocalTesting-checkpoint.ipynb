{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Mar  3 10:33:00 2021\n",
    "\n",
    "@author: jamesmason\n",
    "\"\"\"\n",
    "from ClassFiles.ShapeGenerator import ShapeGenerator\n",
    "from ClassFiles.ChanVese import ChanVese\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "def create(times=1,size=(128, 128), cleansaave     =\"images/clean/clean_\", \n",
    "                                    dirtysave      =\"images/dirty/dirty_\",\n",
    "                                    chansave       =\"images/chan-vese/chanvese_\",\n",
    "                                    datacleansaave =\"data/clean/clean_\", \n",
    "                                    datadirtysave  =\"data/dirty/dirty_\",\n",
    "                                    datachansave   =\"data/chan-vese/chanvese_\"):\n",
    "    \n",
    "    for i in range(times):\n",
    "        e = datetime.datetime.now().strftime(\"%m_%d_%H_%M_%S_%f\")\n",
    "        #create a clean image\n",
    "        shapes = ShapeGenerator(128, 128)\n",
    "        shapes.add_polygon(times=3)\n",
    "        shapes.add_ellipse(times=3)\n",
    "        #save in clean\n",
    "        shapes.image.save(fp = cleansaave+e+\".png\", format = 'PNG')\n",
    "        np.save(file = datacleansaave+e , arr = np.array(shapes.image)/255)\n",
    "        #add noise\n",
    "        shapes.add_holes(40)\n",
    "        shapes.add_noise()\n",
    "        #save in dirty \n",
    "        shapes.image.save(fp = dirtysave+e+\".png\", format = 'PNG')\n",
    "        np.save(file = datadirtysave+e , arr = np.array(shapes.image)/255)\n",
    "        #apply chan-vese\n",
    "        shapes = ChanVese(shapes.image)\n",
    "        shapes.run(steps = 400,show_iterations=False)\n",
    "        #save in chan-vese\n",
    "        np.save(file = datachansave+e , arr = shapes.u)\n",
    "        im = Image.fromarray(255*shapes.u).convert(\"L\")\n",
    "        im.save(fp =  chansave+e+\".png\", format = 'PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Mar  3 10:33:00 2021\n",
    "\n",
    "@author: jamesmason\n",
    "\"\"\"\n",
    "from ClassFiles.ShapeGenerator import ShapeGenerator\n",
    "from ClassFiles.ChanVese import ChanVese\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "def create(times=1,size=(128, 128), cleansaave     =\"eval/images/clean/clean_\", \n",
    "                                    dirtysave      =\"eval/images/dirty/dirty_\",\n",
    "                                    chansave       =\"eval/images/chan-vese/chanvese_\",\n",
    "                                    datacleansaave =\"eval/data/clean/clean_\", \n",
    "                                    datadirtysave  =\"eval/data/dirty/dirty_\",\n",
    "                                    datachansave   =\"eval/data/chan-vese/chanvese_\"):\n",
    "    \n",
    "    for i in range(times):\n",
    "        e = datetime.datetime.now().strftime(\"%m_%d_%H_%M_%S_%f\")\n",
    "        #create a clean image\n",
    "        shapes = ShapeGenerator(128, 128)\n",
    "        shapes.add_polygon(times=3)\n",
    "        shapes.add_ellipse(times=3)\n",
    "        #save in clean\n",
    "        shapes.image.save(fp = cleansaave+e+\".png\", format = 'PNG')\n",
    "        np.save(file = datacleansaave+e , arr = np.array(shapes.image)/255)\n",
    "        #add noise\n",
    "        shapes.add_holes(40)\n",
    "        shapes.add_noise()\n",
    "        #save in dirty \n",
    "        shapes.image.save(fp = dirtysave+e+\".png\", format = 'PNG')\n",
    "        np.save(file = datadirtysave+e , arr = np.array(shapes.image)/255)\n",
    "        #apply chan-vese\n",
    "        shapes = ChanVese(shapes.image)\n",
    "        shapes.run(steps = 400,show_iterations=False)\n",
    "        #save in chan-vese\n",
    "        np.save(file = datachansave+e , arr = shapes.u)\n",
    "        im = Image.fromarray(255*shapes.u).convert(\"L\")\n",
    "        im.save(fp =  chansave+e+\".png\", format = 'PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "create(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from ClassFiles.DataLoader import get_generated_dataloader\n",
    "\n",
    "\"\"\" for now we assume we generate all training data beforehand as numpy arrays.\n",
    "we then convert to pytorch tensors and store on the cpu memory, converting to\n",
    "gpu memory when needed. If memory or speed becomes an issue we could rewrite the\n",
    "training data generation on the gpu using pytorch's linear algebra, and generate\n",
    "it in situ while training \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def train(\n",
    "    NN,\n",
    "    epochs=100,\n",
    "    batch_size=20,\n",
    "    mu=20,\n",
    "    lr=0.0001,\n",
    "    device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    NN is the neural network, e.g. could initialise by NN = SebastianConvNet(1, 256, 256)\n",
    "\n",
    "    groundtruth_numpy is numpy array of [batchsize, image_channels,\n",
    "    image_height, image_width] groundtruth segmentations\n",
    "\n",
    "    chanvese_numpy is numpy array of [batchsize, image_channels, image_height,\n",
    "    image_width] chanvese segmentations\n",
    "\n",
    "    The two datasets do not have to contain corresponding images for the purpose\n",
    "    of training (see paper for why) INFACT THEY SHOULDN'T (should probably\n",
    "    incorporate this by shuffling beforehand, or could potentially include a\n",
    "    shuffle command here)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    NN.to(device)\n",
    "    # not sure why Sebastian doesn't use Adam, but hey\n",
    "    optimiser = optim.RMSprop(NN.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "    groundtruth_loader = get_generated_dataloader('train', 'clean', batch_size)\n",
    "    chanvese_loader = get_generated_dataloader('train', 'chan-vese', batch_size)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        \"\"\"\n",
    "        haven't got a log keeping track of training progress at the moment\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        assert len(groundtruth_loader) == len(chanvese_loader)\n",
    "        \n",
    "        groundtruth_iter = iter(groundtruth_loader)\n",
    "        chanvese_iter = iter(chanvese_loader)\n",
    "        \n",
    "        for i in range(len(groundtruth_loader)):\n",
    "            groundtruth_batch = groundtruth_iter.next()[0]\n",
    "            chanvese_batch = chanvese_iter.next()[0]\n",
    "            \n",
    "            assert groundtruth_batch.size() == chanvese_batch.size()\n",
    "\n",
    "            groundtruth_batch = groundtruth_batch.to(device)\n",
    "            chanvese_batch = chanvese_batch.to(device)\n",
    "\n",
    "            batchsize = groundtruth_batch.size(0)\n",
    "\n",
    "            # REVIEW: Unsqueezing over the 1-axis is enough for batchwise multiplication\n",
    "            epsilon = torch.rand([batchsize], device=device).unsqueeze(1).unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "            intermediate_batch = (\n",
    "                epsilon * groundtruth_batch + (1 - epsilon) * chanvese_batch\n",
    "            )  # [batchsize, channels, height, width]\n",
    "            intermediate_batch.requires_grad = True\n",
    "\n",
    "            # apply the neural network\n",
    "            groundtruth_out = NN(groundtruth_batch)  # [batchsize]\n",
    "            chanvese_out = NN(chanvese_batch)  # [batchsize]\n",
    "            intermediate_out = NN(intermediate_batch)  # [batchsize]\n",
    "\n",
    "            # REVIEW: Why mean() and not sum()?\n",
    "            # calculate the loss\n",
    "            wasserstein_loss = (groundtruth_out - chanvese_out).mean()  # [1]\n",
    "\n",
    "            # Set 'create_graph=True' so we can backprop a function of the\n",
    "            # gradient (--> second derivatives). This is needed for implementing\n",
    "            # the approximate 1-Lipschitz constraint.\n",
    "            # REVIEW: Maybe we should use \"retain_graph\"?\n",
    "            gradient = torch.autograd.grad(\n",
    "                intermediate_out.sum(), intermediate_batch, create_graph=True\n",
    "            )[0]\n",
    "            # --> [batchsize, channels, height, width]\n",
    "\n",
    "            gradient_loss = (\n",
    "                (F.relu(gradient.square().sum((1, 2, 3)).sqrt() - 1)).square().mean()\n",
    "            )  # [1]\n",
    "            loss = wasserstein_loss + mu * gradient_loss  # [1]\n",
    "\n",
    "            # backprop step\n",
    "            # no need to zero the gradients of the intermediate point, since it\n",
    "            # is reinitialised each batch\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        \n",
    "        print('done epoch')\n",
    "    \n",
    "    #torch.save(NN.state_dict(), 'shapesNN_trained')\n",
    "\n",
    "    return NN.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ClassFiles.networks import SebastianConvNet\n",
    "NN = SebastianConvNet(1, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n",
      "done epoch\n"
     ]
    }
   ],
   "source": [
    "NN_trained = train(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_groundtruth_loader = get_generated_dataloader('eval', 'clean', batch_size = 20)\n",
    "eval_chanvese_loader = get_generated_dataloader('eval', 'chan-vese', batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_groundtruth_batch = iter(eval_groundtruth_loader).next()[0]\n",
    "eval_chanvese_batch = iter(eval_chanvese_loader).next()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-45.81377410888672 -38.07417678833008\n"
     ]
    }
   ],
   "source": [
    "print(NN_trained(eval_groundtruth_batch).mean().item(), NN_trained(eval_chanvese_batch).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-45.640769958496094 -31.28079605102539\n"
     ]
    }
   ],
   "source": [
    "print(NN_trained(eval_groundtruth_batch).mean().item(), NN_trained(eval_chanvese_batch).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-48.58849334716797 -30.307575225830078\n"
     ]
    }
   ],
   "source": [
    "print(NN_trained(eval_groundtruth_batch).mean().item(), NN_trained(eval_chanvese_batch).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from ClassFiles.DataLoader import get_generated_dataloader\n",
    "\n",
    "\"\"\" for now we assume we generate all training data beforehand as numpy arrays.\n",
    "we then convert to pytorch tensors and store on the cpu memory, converting to\n",
    "gpu memory when needed. If memory or speed becomes an issue we could rewrite the\n",
    "training data generation on the gpu using pytorch's linear algebra, and generate\n",
    "it in situ while training \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def train(\n",
    "    NN,\n",
    "    epochs=100,\n",
    "    batch_size=20,\n",
    "    mu=20,\n",
    "    lr=0.0001,\n",
    "    device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    NN is the neural network, e.g. could initialise by NN = SebastianConvNet(1, 256, 256)\n",
    "\n",
    "    groundtruth_numpy is numpy array of [batchsize, image_channels,\n",
    "    image_height, image_width] groundtruth segmentations\n",
    "\n",
    "    chanvese_numpy is numpy array of [batchsize, image_channels, image_height,\n",
    "    image_width] chanvese segmentations\n",
    "\n",
    "    The two datasets do not have to contain corresponding images for the purpose\n",
    "    of training (see paper for why) INFACT THEY SHOULDN'T (should probably\n",
    "    incorporate this by shuffling beforehand, or could potentially include a\n",
    "    shuffle command here)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    NN.to(device)\n",
    "    # not sure why Sebastian doesn't use Adam, but hey\n",
    "    optimiser = optim.RMSprop(NN.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "    groundtruth_loader = get_generated_dataloader('train', 'clean', batch_size)\n",
    "    chanvese_loader = get_generated_dataloader('train', 'chan-vese', batch_size)\n",
    "    \n",
    "    eval_groundtruth_loader = get_generated_dataloader('eval', 'clean', batch_size = 100)\n",
    "    eval_chanvese_loader = get_generated_dataloader('eval', 'chan-vese', batch_size = 100)\n",
    "    \n",
    "    \n",
    "    eval_groundtruth_batch = iter(eval_groundtruth_loader).next()[0].to(device)\n",
    "    eval_chanvese_batch = iter(eval_chanvese_loader).next()[0].to(device)\n",
    "    with torch.no_grad():\n",
    "        groundtruth_mean_value = NN(eval_groundtruth_batch).mean().item()\n",
    "        chanvese_mean_value = NN(eval_chanvese_batch).mean().item()\n",
    "    print('untrained performance', groundtruth_mean_value, chanvese_mean_value)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \"\"\"\n",
    "        haven't got a log keeping track of training progress at the moment\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        assert len(groundtruth_loader) == len(chanvese_loader)\n",
    "        \n",
    "        groundtruth_iter = iter(groundtruth_loader)\n",
    "        chanvese_iter = iter(chanvese_loader)\n",
    "        \n",
    "        for i in range(len(groundtruth_loader)):\n",
    "            groundtruth_batch = groundtruth_iter.next()[0]\n",
    "            chanvese_batch = chanvese_iter.next()[0]\n",
    "            \n",
    "            assert groundtruth_batch.size() == chanvese_batch.size()\n",
    "\n",
    "            groundtruth_batch = groundtruth_batch.to(device)\n",
    "            chanvese_batch = chanvese_batch.to(device)\n",
    "\n",
    "            batchsize = groundtruth_batch.size(0)\n",
    "\n",
    "            # REVIEW: Unsqueezing over the 1-axis is enough for batchwise multiplication\n",
    "            epsilon = torch.rand([batchsize], device=device).unsqueeze(1).unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "            intermediate_batch = (\n",
    "                epsilon * groundtruth_batch + (1 - epsilon) * chanvese_batch\n",
    "            )  # [batchsize, channels, height, width]\n",
    "            intermediate_batch.requires_grad = True\n",
    "\n",
    "            # apply the neural network\n",
    "            groundtruth_out = NN(groundtruth_batch)  # [batchsize]\n",
    "            chanvese_out = NN(chanvese_batch)  # [batchsize]\n",
    "            intermediate_out = NN(intermediate_batch)  # [batchsize]\n",
    "\n",
    "            # REVIEW: Why mean() and not sum()?\n",
    "            # calculate the loss\n",
    "            wasserstein_loss = (groundtruth_out - chanvese_out).mean()  # [1]\n",
    "\n",
    "            # Set 'create_graph=True' so we can backprop a function of the\n",
    "            # gradient (--> second derivatives). This is needed for implementing\n",
    "            # the approximate 1-Lipschitz constraint.\n",
    "            # REVIEW: Maybe we should use \"retain_graph\"?\n",
    "            gradient = torch.autograd.grad(\n",
    "                intermediate_out.sum(), intermediate_batch, create_graph=True\n",
    "            )[0]\n",
    "            # --> [batchsize, channels, height, width]\n",
    "\n",
    "            gradient_loss = (\n",
    "                (F.relu(gradient.square().sum((1, 2, 3)).sqrt() - 1)).square().mean()\n",
    "            )  # [1]\n",
    "            loss = wasserstein_loss + mu * gradient_loss  # [1]\n",
    "\n",
    "            # backprop step\n",
    "            # no need to zero the gradients of the intermediate point, since it\n",
    "            # is reinitialised each batch\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        \n",
    "        eval_groundtruth_batch = iter(eval_groundtruth_loader).next()[0].to(device)\n",
    "        eval_chanvese_batch = iter(eval_chanvese_loader).next()[0].to(device)\n",
    "        with torch.no_grad():\n",
    "            groundtruth_mean_value = NN(eval_groundtruth_batch).mean().item()\n",
    "            chanvese_mean_value = NN(eval_chanvese_batch).mean().item()\n",
    "        print('done epoch', groundtruth_mean_value, chanvese_mean_value)\n",
    "    \n",
    "    #torch.save(NN.state_dict(), 'shapesNN_trained2')\n",
    "\n",
    "    return NN.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "untrained performance 0.038864150643348694 0.038880396634340286\n",
      "done epoch 12.618231773376465 18.813182830810547\n",
      "done epoch 63.58605194091797 71.1834487915039\n",
      "done epoch 15.920390129089355 22.874990463256836\n",
      "done epoch 49.21866226196289 56.95798110961914\n",
      "done epoch 45.18325424194336 53.36827087402344\n",
      "done epoch 16.57179832458496 24.142093658447266\n",
      "done epoch 4.840348243713379 12.920366287231445\n",
      "done epoch -12.929286003112793 -4.847332000732422\n",
      "done epoch 7.324130535125732 16.498716354370117\n",
      "done epoch 12.941987991333008 22.389266967773438\n",
      "done epoch 2.5564610958099365 9.952454566955566\n",
      "done epoch 22.267440795898438 31.339984893798828\n",
      "done epoch 16.84214973449707 26.06429672241211\n",
      "done epoch 20.55097770690918 29.98940086364746\n",
      "done epoch 21.843690872192383 31.192262649536133\n",
      "done epoch 12.76811695098877 18.48917579650879\n",
      "done epoch 13.273622512817383 22.387664794921875\n",
      "done epoch -0.3863922655582428 8.586007118225098\n",
      "done epoch 19.88096809387207 30.033798217773438\n",
      "done epoch 3.3203985691070557 12.974955558776855\n",
      "done epoch 6.798630237579346 14.866791725158691\n",
      "done epoch 27.78396987915039 37.431922912597656\n",
      "done epoch 22.418434143066406 31.977558135986328\n",
      "done epoch 21.37936019897461 31.86383628845215\n",
      "done epoch 14.534843444824219 24.458913803100586\n",
      "done epoch 17.73423194885254 27.599082946777344\n",
      "done epoch 17.76401138305664 28.42345428466797\n",
      "done epoch 15.369997024536133 24.678129196166992\n",
      "done epoch 3.1206774711608887 13.469244956970215\n",
      "done epoch 17.447994232177734 26.494604110717773\n",
      "done epoch 16.48859977722168 25.426565170288086\n",
      "done epoch 18.616697311401367 29.136714935302734\n",
      "done epoch 12.481947898864746 22.650041580200195\n",
      "done epoch 20.022994995117188 29.740013122558594\n",
      "done epoch 20.697799682617188 31.842679977416992\n",
      "done epoch -5.266409397125244 4.635280132293701\n",
      "done epoch 25.426965713500977 37.08378982543945\n",
      "done epoch 14.71238899230957 25.84564971923828\n",
      "done epoch 13.129412651062012 23.483505249023438\n",
      "done epoch 17.289655685424805 28.28470230102539\n",
      "done epoch 12.490814208984375 23.136804580688477\n",
      "done epoch 6.6533355712890625 16.78489875793457\n",
      "done epoch 11.2254638671875 22.227706909179688\n",
      "done epoch 12.927722930908203 23.323434829711914\n",
      "done epoch 14.78596019744873 24.25581932067871\n",
      "done epoch 3.4507088661193848 11.190007209777832\n",
      "done epoch 11.300956726074219 21.893651962280273\n",
      "done epoch 17.092103958129883 28.34181022644043\n",
      "done epoch 9.630213737487793 19.647706985473633\n",
      "done epoch 17.529003143310547 28.467418670654297\n",
      "done epoch 9.560508728027344 20.191997528076172\n",
      "done epoch 23.94072151184082 36.05665588378906\n",
      "done epoch 15.08686637878418 25.528217315673828\n",
      "done epoch 18.243642807006836 28.023624420166016\n",
      "done epoch 19.764751434326172 30.487594604492188\n",
      "done epoch 10.642793655395508 20.303030014038086\n",
      "done epoch 11.631915092468262 22.590909957885742\n",
      "done epoch 10.879286766052246 20.744348526000977\n",
      "done epoch 18.41177749633789 30.3018741607666\n",
      "done epoch 8.816929817199707 18.786056518554688\n",
      "done epoch 7.423163414001465 18.366113662719727\n",
      "done epoch 18.04773712158203 29.59109878540039\n",
      "done epoch 15.05903148651123 27.499446868896484\n",
      "done epoch 3.3539953231811523 15.065327644348145\n",
      "done epoch 7.227090835571289 18.784648895263672\n",
      "done epoch 7.27715539932251 18.54861068725586\n",
      "done epoch 6.738347053527832 18.656930923461914\n",
      "done epoch 12.729009628295898 23.989469528198242\n",
      "done epoch 8.793916702270508 19.523578643798828\n",
      "done epoch 1.0489357709884644 12.906159400939941\n",
      "done epoch 1.0886231660842896 11.961283683776855\n",
      "done epoch -0.22909988462924957 10.676532745361328\n",
      "done epoch 7.668595790863037 19.048120498657227\n",
      "done epoch -0.30091366171836853 11.217141151428223\n",
      "done epoch 7.087810516357422 19.917583465576172\n",
      "done epoch 7.477637767791748 17.95115852355957\n",
      "done epoch -6.487707138061523 3.8156938552856445\n",
      "done epoch -2.2463512420654297 9.224870681762695\n",
      "done epoch 11.889765739440918 22.61054229736328\n",
      "done epoch 6.778377532958984 18.88112449645996\n",
      "done epoch -1.5784261226654053 9.108003616333008\n",
      "done epoch -12.583696365356445 -0.004351424984633923\n",
      "done epoch 7.594628810882568 17.32082748413086\n",
      "done epoch 4.148401260375977 16.687734603881836\n",
      "done epoch 1.207132339477539 13.112109184265137\n",
      "done epoch 7.645359992980957 20.91012954711914\n",
      "done epoch 4.466878414154053 16.822879791259766\n",
      "done epoch 4.733827590942383 16.501115798950195\n",
      "done epoch 3.996840476989746 13.53750228881836\n",
      "done epoch -1.2320219278335571 8.988738059997559\n",
      "done epoch 0.31425797939300537 11.511038780212402\n",
      "done epoch 0.9484506249427795 12.764662742614746\n",
      "done epoch 4.318285942077637 16.13926124572754\n",
      "done epoch 3.4110865592956543 14.06610107421875\n",
      "done epoch 3.8694255352020264 13.929008483886719\n",
      "done epoch 4.642940998077393 15.471014976501465\n",
      "done epoch 13.88308334350586 24.34629249572754\n",
      "done epoch -4.795783996582031 6.534531593322754\n",
      "done epoch -3.2489519119262695 7.750180721282959\n",
      "done epoch -5.763635158538818 5.858601093292236\n"
     ]
    }
   ],
   "source": [
    "from ClassFiles.networks import SebastianConvNet\n",
    "NN = SebastianConvNet(1, 128, 128)\n",
    "NN_trained2 = train(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "the regularisation parameter lambda CANNOT be initialised in the same way for segmentation as for denoising\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "the function unreg_mini in Sebastian's paper shouldn't need to be redone here, as chanvese should already be a good starting point for reconstruction\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "must recalculate data fitting term in pytorch, so we can compute gradients\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "I THINK THIS IS THE IMPLEMENTATION MIKE WANTED\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def data_fitting(\n",
    "    chanvese_batch,\n",
    "    noisy_batch,\n",
    "    lambda_chanvese=1,\n",
    "    threshold=0.5,\n",
    "    c1=None,\n",
    "    c2=None,\n",
    "    alpha=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    chanvese_batch & noisy_batch must a torch.tensor (ideally on gpu) of size [batchsize, 1, height, width]\n",
    "    noisy_batch contains the corresponding noisy images to chanvese_batch\n",
    "    threshold is used to find the segmentation boundary (for the purpose of calculating c1, c2) from chanvese_batch\n",
    "    \"\"\"\n",
    "    assert chanvese_batch.size() == noisy_batch.size()\n",
    "    assert chanvese_batch.size(1) == 1  # require greyscale image, i.e. only one channel\n",
    "\n",
    "    # calculate c1, c2 implicity from u\n",
    "    \"\"\"\n",
    "    WE DO NOT want to backprop along c1, c2 when performing the reconstruction (algorithm 2), only relevant when c1, c2 are calculated implicitly\n",
    "    Actually not a problem, since no operation below is going to preserve requires_grad = true\n",
    "    \"\"\"\n",
    "    if c1 == None:\n",
    "        c1 = (noisy_batch * (chanvese_batch > threshold)).mean((1, 2, 3))  # [batchsize]\n",
    "    if c2 == None:\n",
    "        c2 = (noisy_batch * (chanvese_batch <= threshold)).mean(\n",
    "            (1, 2, 3)\n",
    "        )  # [batchsize]\n",
    "\n",
    "    chanvese_term = lambda_chanvese * (\n",
    "        (noisy_batch - c1.unsqueeze(1).unsqueeze(2).unsqueeze(3)).square()\n",
    "        - (noisy_batch - c2.unsqueeze(1).unsqueeze(2).unsqueeze(3)).square()\n",
    "    )  # [batchsize, 1, height, width]\n",
    "\n",
    "    # calculate alpha implicity from u, lambda, c1, and c2?\n",
    "    \"\"\"\n",
    "    WE DO NOT want to backprop along alpha when performing the reconstruction (algorithm 2), only relevant when alpha is calculated implicitly\n",
    "    hence .detach() below\n",
    "    \"\"\"\n",
    "    if alpha == None:\n",
    "        alpha = (\n",
    "            chanvese_term.detach().abs().max(3)[0].max(2)[0].max(1)[0]\n",
    "        )  # [batchsize]\n",
    "\n",
    "    penality_term = 2 * (\n",
    "        (chanvese_batch - 0.5).abs() - 1\n",
    "    )  # [batchsize, 1, height, width]\n",
    "    penality_term = penality_term * (penality_term > 0)  # [batchsize, 1, height, width]\n",
    "\n",
    "    \"\"\"\n",
    "    integral over domain is just done by taking the mean, should just correspond to scaling lambda_reg accordingly in reconstruct (below)\n",
    "    \"\"\"\n",
    "    datafitting_term = (\n",
    "        chanvese_term * chanvese_batch\n",
    "        + alpha.unsqueeze(1).unsqueeze(2).unsqueeze(3) * penality_term\n",
    "    ).mean(\n",
    "        (1, 2, 3)\n",
    "    )  # [batchsize]\n",
    "\n",
    "    return datafitting_term  # [batchsize]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "ALGORITHM 2:\n",
    "simultaneously perform a number of gradient descent steps on a full batch of chanvese segmentations, or already partialy reconstructed segmentations\n",
    "(both would take the argument chanvese_batch below)\n",
    "noisy_batch contains the corresponding noisy images to chanvese_batch (for the purpose of the datafitting term above)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def reconstruct(\n",
    "    chanvese_batch, noisy_batch, NN, lambda_reg, epsilon, reconstruction_steps=1\n",
    "):\n",
    "    \"\"\"\n",
    "    chanvese_batch & noisy_batch must be a torch.tensor of size [batchsize, channels, height, width]\n",
    "    NN is the learnt regulariser\n",
    "    lambda_reg is how much we weight the regularising term (not the datafitting term) when reconstructing the solution according to algorithm 2\n",
    "    \"\"\"\n",
    "    device = next(NN.parameters()).device  # trick to find device NN is stored on\n",
    "    reconstructed_batch = chanvese_batch.to(\n",
    "        device\n",
    "    ).detach()  # transfer chanvese_batch to same device NN is stored on, detach just incase\n",
    "    noisy_batch_copy = noisy_batch.to(\n",
    "        device\n",
    "    )  # transfer noisy_batch to same device NN is stored on\n",
    "\n",
    "    for i in range(reconstruction_steps):\n",
    "        reconstructed_batch.requires_grad = True  # set requires_grad to True, gradients are initialised at zero, and entire backprop graph will be recreated (not the most efficient way, as autograd graph has to be recreated each time)\n",
    "\n",
    "        \"\"\"\n",
    "        data_fitting function not yet implemented\n",
    "        \"\"\"\n",
    "        datafitting = data_fitting(reconstructed_batch, noisy_batch_copy)  # [batchsize]\n",
    "        regularising = NN(reconstructed_batch)  # [batchsize]\n",
    "\n",
    "        error = datafitting + lambda_reg * regularising  # [batchsize]\n",
    "        error = error.sum()  # [1], trick to compute all gradients in one go\n",
    "\n",
    "        gradients = torch.autograd.grad(error, reconstructed_batch)[0]\n",
    "        reconstructed_batch = (\n",
    "            reconstructed_batch - epsilon * gradients\n",
    "        ).detach()  # detaching from previous autograd which also sets requires_grad to False\n",
    "\n",
    "    return (\n",
    "        reconstructed_batch.to(chanvese_batch.device),\n",
    "        noisy_batch,\n",
    "    )  # send back to original device\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "a quick function for evaluating the quality of the reconstructed segmentation according to the L2 difference between it and groundtruth\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def quality(reconstructed_batch, groundtruth_batch):\n",
    "    \"\"\"\n",
    "    reconstructed_batch, and groundtruth_batch must be torch.tensors of the same size [batchsize, channels, height, width]\n",
    "    \"\"\"\n",
    "    return (\n",
    "        (reconstructed_batch - groundtruth_batch).square().sum((1, 2, 3)).sqrt()\n",
    "    )  # [batchsize]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "analogue of Sebastian's function log_minimum (except without storing any data in logs), which keeps reconstructing solutions until their quality (as defined above, i.e. requiring knowledge of the ground truth) no longer keeps decreasing\n",
    "\n",
    "idea is to use this to evaluate performance of NN\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def minimum(chanvese_batch, noisy_batch, groundtruth_batch, NN, lambda_reg, epsilon):\n",
    "\n",
    "    assert chanvese_batch.size() == noisy_batch.size()\n",
    "    assert chanvese_batch.size() == groundtruth_batch.size()\n",
    "    assert chanvese_batch.device == noisy_batch.device\n",
    "    assert chanvese_batch.device == groundtruth_batch.device\n",
    "    batchsize = chanvese_batch.size(0)\n",
    "    device = chanvese_batch.device\n",
    "\n",
    "    todo_mask = torch.ones([batchsize], dtype=torch.bool, device=device)\n",
    "    chanvese_todo = chanvese_batch\n",
    "    noisy_todo = noisy_batch\n",
    "    groundtruth_todo = groundtruth_batch\n",
    "    quality_prev = torch.full([batchsize], float(\"inf\"), device=device)\n",
    "    minimum_batch = torch.empty_like(chanvese_todo)\n",
    "    final_quality = torch.empty_like(quality_prev)\n",
    "    steps = torch.zeros_like(quality_prev)\n",
    "\n",
    "    while todo_mask.sum():\n",
    "        print(todo_mask.sum())\n",
    "        steps += todo_mask\n",
    "\n",
    "        chanvese_todo = reconstruct(chanvese_todo, noisy_todo, NN, lambda_reg, epsilon)[0]\n",
    "        quality_new = quality(chanvese_todo, groundtruth_todo)\n",
    "        done_mask = quality_new > quality_prev\n",
    "\n",
    "        done_mask_unravel = torch.zeros_like(todo_mask).masked_scatter(\n",
    "            todo_mask, done_mask\n",
    "        )\n",
    "\n",
    "        minimum_batch.masked_scatter_(\n",
    "            done_mask_unravel.unsqueeze(1)\n",
    "            .unsqueeze(2)\n",
    "            .unsqueeze(3)\n",
    "            .expand(minimum_batch.size()),\n",
    "            chanvese_todo.masked_select(\n",
    "                done_mask.unsqueeze(1)\n",
    "                .unsqueeze(2)\n",
    "                .unsqueeze(3)\n",
    "                .expand(chanvese_todo.size())\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        final_quality.masked_scatter_(\n",
    "            done_mask_unravel, quality_new.masked_select(done_mask)\n",
    "        )\n",
    "        todo_mask.masked_fill_(done_mask_unravel, False)\n",
    "\n",
    "        chanvese_todo = chanvese_todo.masked_select(\n",
    "            done_mask.logical_not()\n",
    "            .unsqueeze(1)\n",
    "            .unsqueeze(2)\n",
    "            .unsqueeze(3)\n",
    "            .expand(chanvese_todo.size())\n",
    "        ).view(\n",
    "            -1, chanvese_batch.size(1), chanvese_batch.size(2), chanvese_batch.size(3)\n",
    "        )\n",
    "        noisy_todo = noisy_todo.masked_select(\n",
    "            done_mask.logical_not()\n",
    "            .unsqueeze(1)\n",
    "            .unsqueeze(2)\n",
    "            .unsqueeze(3)\n",
    "            .expand(noisy_todo.size())\n",
    "        ).view(\n",
    "            -1, chanvese_batch.size(1), chanvese_batch.size(2), chanvese_batch.size(3)\n",
    "        )\n",
    "        groundtruth_todo = groundtruth_todo.masked_select(\n",
    "            done_mask.logical_not()\n",
    "            .unsqueeze(1)\n",
    "            .unsqueeze(2)\n",
    "            .unsqueeze(3)\n",
    "            .expand(groundtruth_todo.size())\n",
    "        ).view(\n",
    "            -1, chanvese_batch.size(1), chanvese_batch.size(2), chanvese_batch.size(3)\n",
    "        )\n",
    "        quality_prev = quality_new.masked_select(done_mask.logical_not())\n",
    "\n",
    "    return (\n",
    "        minimum_batch,\n",
    "        final_quality,\n",
    "        steps,\n",
    "    )  # outputs the final (optimal) reconstruction, their corresponding quality, and the reconstruction steps required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_groundtruth_loader = get_generated_dataloader('eval', 'clean', batch_size = 100, shuffle = False)\n",
    "eval_chanvese_loader = get_generated_dataloader('eval', 'chan-vese', batch_size = 100, shuffle = False)\n",
    "eval_noisy_loader = get_generated_dataloader('eval', 'dirty', batch_size = 100, shuffle = False)\n",
    "\n",
    "eval_groundtruth_batch = iter(eval_groundtruth_loader).next()[0]\n",
    "eval_chanvese_batch = iter(eval_chanvese_loader).next()[0]\n",
    "eval_noisy_batch = iter(eval_noisy_loader).next()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_batch, _ = reconstruct(eval_chanvese_batch, eval_noisy_batch, NN_trained2, 1, 0.001, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20.7308, 22.2587, 21.2586, 20.2870, 21.7970, 25.1236, 21.2068, 18.8730,\n",
       "        26.8303, 22.2677, 31.8682, 21.3523, 24.3287, 18.9921, 23.1867, 17.1488,\n",
       "        21.1736, 20.3297, 20.7431, 20.4539, 25.2307, 21.0587, 23.1987, 24.5889,\n",
       "        23.3309, 17.5778, 24.2103, 20.2828, 18.7939, 20.5546, 24.8759, 26.5289,\n",
       "        17.9498, 21.6536, 19.8809, 19.0416, 18.6636, 29.5160, 19.5933, 22.6014,\n",
       "        18.9797, 22.4489, 21.6593, 18.8388, 23.4502, 23.9608, 18.1446, 22.6261,\n",
       "        24.9773, 17.1516, 23.6668, 20.2068, 22.4793, 17.1010, 20.3928, 22.2331,\n",
       "        17.3984, 27.0341, 18.4000, 28.0716, 25.2354, 23.7372, 25.0803, 25.4750,\n",
       "        20.9388, 21.2452, 26.2204, 20.4299, 19.3649, 23.1272, 18.5059, 22.4491,\n",
       "        23.8683, 25.7508, 23.3741, 19.3996, 20.1539, 20.5131, 18.9185, 28.9504,\n",
       "        18.9385, 24.8252, 18.1016, 26.8381, 24.4966, 26.6884, 19.7876, 20.4098,\n",
       "        23.4061, 19.1060, 21.4435, 21.5396, 20.2280, 18.2687, 18.0217, 16.4721,\n",
       "        20.0200, 19.8940, 19.0621, 24.3262])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quality(minimum_batch, eval_groundtruth_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19.2556, 21.5176, 19.8189, 18.7138, 21.1041, 23.9811, 19.9073, 16.9417,\n",
       "        26.8937, 20.6464, 31.6227, 20.3902, 23.7194, 18.3315, 22.3634, 15.5090,\n",
       "        19.6037, 18.9297, 18.8746, 18.9002, 24.0845, 19.4287, 22.6050, 23.9401,\n",
       "        22.2961, 15.8771, 23.4152, 18.4406, 17.6938, 19.0591, 24.0654, 25.6187,\n",
       "        16.4093, 20.3504, 19.0126, 17.4013, 17.1774, 28.7044, 18.3316, 21.9211,\n",
       "        17.2012, 21.3382, 20.4999, 17.7858, 22.0503, 23.6145, 16.5793, 21.5948,\n",
       "        24.0078, 15.7314, 21.9132, 18.7845, 21.2659, 15.0023, 19.0546, 20.8455,\n",
       "        16.1690, 26.3345, 16.9159, 27.3111, 24.2817, 23.1646, 23.9969, 24.7396,\n",
       "        19.6489, 20.1087, 25.3276, 19.0618, 18.0218, 22.2856, 17.0062, 20.7066,\n",
       "        22.5429, 24.8365, 22.1405, 18.0067, 18.8572, 18.9382, 17.6190, 28.4396,\n",
       "        16.6455, 23.8683, 16.7484, 26.1403, 23.6404, 26.1381, 18.3444, 18.5999,\n",
       "        21.9343, 17.3227, 20.0019, 20.6936, 19.0761, 17.1766, 16.4064, 15.3241,\n",
       "        18.4786, 18.6037, 17.7078, 23.4370])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quality(eval_chanvese_batch, eval_groundtruth_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.556939601898193 -5.763604640960693 5.858641147613525\n"
     ]
    }
   ],
   "source": [
    "print(NN_trained2(minimum_batch).mean().item(), NN_trained2(eval_groundtruth_batch).mean().item(), NN_trained2(eval_chanvese_batch).mean().item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_batch_numpy = minimum_batch.numpy()\n",
    "minimum_batch_numpy_squeeze = minimum_batch_numpy.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "for i in range(100):\n",
    "    im = Image.fromarray(255*minimum_batch_numpy_squeeze[i]).convert(\"L\")\n",
    "    im.save(fp =  \"eval/images/reconstructed/reconstructed_\"+str(i).zfill(2)+\".png\", format = 'PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
